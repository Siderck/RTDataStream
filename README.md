# ğŸš€ **Proyecto de IngenierÃ­a de Datos: TransmisiÃ³n de Datos en Tiempo Real**

## ğŸ“– **Tabla de Contenidos**
- [ğŸ“¢ IntroducciÃ³n](#introducciÃ³n)
- [ğŸ› ï¸ Arquitectura del Sistema](#arquitectura-del-sistema)
- [ğŸ¯ Objetivos de Aprendizaje](#objetivos-de-aprendizaje)
- [ğŸ’» TecnologÃ­as Utilizadas](#tecnologÃ­as-utilizadas)
- [ğŸš€ CÃ³mo Empezar](#cÃ³mo-empezar)

---

## ğŸ“¢ **IntroducciÃ³n**
Este proyecto es una guÃ­a integral para construir un pipeline de datos desde cero, abarcando todas las etapas: **ingestiÃ³n, procesamiento y almacenamiento**. Utiliza una arquitectura robusta basada en herramientas modernas como:

- **Apache Airflow**: OrquestaciÃ³n del flujo de datos.
- **Apache Kafka** y **Zookeeper**: Streaming en tiempo real y sincronizaciÃ³n distribuida.
- **Apache Spark**: Procesamiento de datos distribuido.
- **Cassandra** y **PostgreSQL**: Almacenamiento confiable de datos procesados.

Todo el proyecto estÃ¡ completamente **contenedorizado con Docker**, lo que facilita la implementaciÃ³n y escalabilidad.

---

## ğŸ› ï¸ **Arquitectura del Sistema**

![Arquitectura del Sistema](https://www2.udec.cl/~joseparra/2.png)

El proyecto incluye los siguientes componentes:

- **Fuente de Datos**: Utiliza la API `randomuser.me` para generar datos aleatorios de usuarios.
- **Apache Airflow**: Orquesta el pipeline y almacena los datos en una base de datos PostgreSQL.
- **Apache Kafka y Zookeeper**: Transmiten los datos de PostgreSQL al motor de procesamiento.
- **Control Center y Schema Registry**: Supervisan y gestionan los esquemas de los streams de Kafka.
- **Apache Spark**: Procesa los datos con nodos maestro y trabajador.
- **Cassandra**: Almacena los datos procesados para su consulta y anÃ¡lisis.

---

## ğŸ¯ **Objetivos de Aprendizaje**
Con este proyecto, se ponen a prueba las siguientes habilidades:

- Configurar un pipeline de datos con **Apache Airflow**.
- Implementar **streaming de datos en tiempo real** con **Apache Kafka**.
- Sincronizar procesos distribuidos utilizando **Apache Zookeeper**.
- Procesar datos con **Apache Spark**.
- Almacenar datos en bases de datos como **Cassandra** y **PostgreSQL**.
- Contenerizar una arquitectura completa de datos con **Docker**.

---

## ğŸ’» **TecnologÃ­as Utilizadas**

Este proyecto emplea las siguientes herramientas y tecnologÃ­as modernas:

- **Apache Airflow**
- **Python**
- **Apache Kafka**
- **Apache Zookeeper**
- **Apache Spark**
- **Cassandra**
- **PostgreSQL**
- **Docker**

---

## ğŸš€ **CÃ³mo Empezar**

1. Clona el repositorio:
    ```bash
    git clone https://github.com/Siderck/RTDataStream.git
    ```

2. Navega al directorio del proyecto:
    ```bash
    cd RTDataStream
    ```

3. Inicia los servicios con Docker Compose:
    ```bash
    docker-compose up
    ```

4. Accede al servidor de Airflow en tu navegador:  
   [http://localhost:8080](http://localhost:8080)  
